# -*- coding: utf-8 -*-
"""Project1-bbc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NDqBTy-bTUQck909AhbNoeX2hSUhNIyV

# Proyek Pertama : Membuat Model NLP dengan TensorFlow

- Nama : Ilyasa Nanda Rahmadianto
- Username : [ilyasanara16](https://www.dicoding.com/users/ilyasanara16/academies)
### Berikut kriteria submission yang harus Anda penuhi:
1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel.

2. Harus menggunakan LSTM dalam arsitektur model.

3. Harus menggunakan model sequential.

4. Validation set sebesar 20% dari total dataset.

5. Harus menggunakan Embedding.

6. Harus menggunakan fungsi tokenizer.

7. Akurasi dari model minimal 75% pada train set dan validation set.

Klik link untuk mengunduh [Dataset](https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category/data)
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# text processing
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
stopwords.words('english')
import string
string.punctuation
from nltk.stem.porter import PorterStemmer
import re

# utils
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

df = pd.read_csv("C:/Users/ASUS/Downloads/TUGASKU/Dicoding/Lintasarta/Project1-MembuatModelNLPdenganTensorFlow/bbc-text.csv")
df.head()
df

df.category.unique()

df.isna().sum()

df.head()

plt.figure(figsize = (8, 8))
sns.countplot(df['category'])

category = pd.get_dummies(df.category)
df_new = pd.concat([df, category], axis = 1)
df_new = df_new.drop(columns = 'category')

df_new

df_baru = df_new.replace({True: 1, False: 0})
df_baru

def remove_Stopwords(text ):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize( text.lower() )
    sentence = [w for w in words if not w in stop_words]
    return " ".join(sentence)


def lemmatize_text(text):
    wordlist=[]
    lemmatizer = WordNetLemmatizer()
    sentences=sent_tokenize(text)
    for sentence in sentences:
        words=word_tokenize(sentence)
        for word in words:
            wordlist.append(lemmatizer.lemmatize(word))
    return ' '.join(wordlist)

def clean_text(text):
    delete_dict = {sp_character: '' for sp_character in string.punctuation}
    delete_dict[' '] = ' '
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    # Remove words starting with \
    text1 = re.sub(r'\\\w*', '', text1)
    textArr= text1.split()
    text2 = ' '.join([w for w in textArr])

    return text2.lower()

#code untuk menghilangkan hastag, link, mention
def cleanText(text):
    text = text.lower()
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'#', '', text)
    text = re.sub(r'RT[\s]+', '', text)
    text = re.sub(r'https?:\/\/\S+', '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'pic.twitter.com', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'^\s+|\s+?$', '', text)
    text = text.strip()
    return text

def stemSentence(text):
    porter = PorterStemmer()
    token_words=word_tokenize(text)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

df_baru['new_text'] = df_baru['text'].apply(clean_text)
df_baru['new_text'] = df_baru['new_text'].apply(remove_Stopwords)
df_baru['new_text'] = df_baru['new_text'].apply(lemmatize_text)
df_baru['stem_teks'] = df_baru['new_text'].apply(stemSentence)
df_baru['clean_teks'] = df_baru['new_text'].apply(cleanText)

df_baru

df_baru.text[0]

df_baru.new_text[0]

df_baru.stem_teks[0]

df_baru.clean_teks[0]

length = df_baru['clean_teks'].str.len().max()
length, df_baru.columns

teks = df_baru['clean_teks'].values
label = df_baru[['tech', 'business', 'sport', 'entertainment', 'politics']].values
label, teks

teks_train, teks_test, label_train, label_test = train_test_split(teks, label, test_size = 0.2)

tokenizer = Tokenizer(num_words = 10000, oov_token = '<OOV>')
# tokenizer = Tokenizer(num_words = 5000, oov_token = 'x')
tokenizer.fit_on_texts(teks_train)
tokenizer.fit_on_texts(teks_test)

sequences_train = tokenizer.texts_to_sequences(teks_train)
sequences_test = tokenizer.texts_to_sequences(teks_test)

# maxlen = max(len(sequence) for sequence in sequences_train)
maxlen = int(sum(len(sequence) for sequence in sequences_train) / len(sequences_train))
# maxlen = int(np.percentile([len(sequence) for sequence in sequences_train], 90))
maxlen

padded_train = pad_sequences(sequences=sequences_train,
                             maxlen = maxlen,
                             padding = 'post',
                             truncating = 'post')
padded_test = pad_sequences(sequences=sequences_test,
                            maxlen = maxlen,
                            padding = 'post',
                            truncating = 'post')

padded_train

import tensorflow as tf
from tensorflow.keras.layers import Dropout

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=256, input_length=maxlen),
    tf.keras.layers.LSTM(512),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

# model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(input_dim=10000, output_dim=256, input_length=maxlen),
#     tf.keras.layers.LSTM(512),
#     tf.keras.layers.Dense(512, activation='relu'),
#     tf.keras.layers.Dense(254, activation='relu'),
#     tf.keras.layers.Dropout(0.3),
#     tf.keras.layers.Dense(128, activation= 'relu'),
#     tf.keras.layers.Dense(5, activation='softmax')
# ])

# model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(input_dim=10000, output_dim=256, input_length=maxlen),
#     tf.keras.layers.LSTM(512),
#     tf.keras.layers.Dense(256, activation='relu'),
#     tf.keras.layers.Dense(128, activation='relu'),
#     tf.keras.layers.Dropout(0.3),
#     tf.keras.layers.Dense(64, activation= 'relu'),
#     tf.keras.layers.Dense(5, activation='softmax')
# ])

#compile
model.compile(loss ='categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

#cek model

model.summary()

#Callback Function
class accCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.98 and logs.get('val_accuracy') >= 0.98):
            print("\nAccuracy and Val_Accuracy has reached 98%!", "\nEpoch: ", epoch)
            self.model.stop_training = True

callbacks = accCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.2,
    min_lr = 0.000003
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 4,
    verbose = 1,
    mode = 'auto'
)

#latih model
history = model.fit(padded_train, label_train,
                    epochs = 100,
                    batch_size=128,
                    steps_per_epoch = 30,
                    validation_data = (padded_test, label_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )

#plotting

pd.DataFrame(history.history).plot(figsize=(7, 4))
plt.grid(True)
plt.gca().set_ylim(0,3) #sumbu y

plt.show()

import numpy as np
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from collections import Counter

sm = SMOTE(random_state = 42)
X_res, y_res = sm.fit_resample(padded_train, label_train)

Counter(np.argmax(y_res, axis = 1))

#compile
model.compile(loss ='categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

new_history = model.fit(X_res, y_res,
                         batch_size = 128,
                    steps_per_epoch = 30,
                    epochs = 100,
                    validation_data = (padded_test, label_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )

# plot loss
plt.subplot(211)
plt.title('Loss')
plt.plot(new_history.history['loss'], label='train')
plt.plot(new_history.history['val_loss'], label='test')
plt.legend()

# plot accuracy
plt.subplot(212)
plt.title('Accuracy')
plt.plot(new_history.history['accuracy'], label='train')
plt.plot(new_history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

# sanity check
sentence = ["'trend new yorker encount empti supermarket shelf pictur wegman brooklyn soldout onlin grocer foodkick maxdeliveri coronavirusfear shopper stock httpstcogr76pcrlwh httpstcoivmkmsqdt1'"]
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=maxlen, padding='post', truncating='post')
pred = model.predict(padded)
pred

# sanity check actual vs prediction
y_pred = model.predict(padded_test)
y_pred = np.argmax(y_pred, axis = 1)
y_test = np.argmax(label_test, axis = 1)

class_names = ['tech', 'business', 'sport', 'entertainment', 'politics']
actual = [class_names[i] for i in y_test]
prediction = [class_names[i] for i in y_pred]

# # data frame actual vs prediction include text
df_pred = pd.DataFrame({'actual': actual, 'prediction': prediction, 'text': teks_test})
df_pred