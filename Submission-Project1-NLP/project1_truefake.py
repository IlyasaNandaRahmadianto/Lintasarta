# -*- coding: utf-8 -*-
"""Project1-TrueFake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oPxSW8xi-8U9XK9DXBpyvjsCQjKZAzjG

# Proyek Pertama : Membuat Model NLP dengan TensorFlow

- Nama : Ilyasa Nanda Rahmadianto
- Username : [ilyasanara16](https://www.dicoding.com/users/ilyasanara16/academies)
### Berikut kriteria submission yang harus Anda penuhi:
1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel.

2. Harus menggunakan LSTM dalam arsitektur model.

3. Harus menggunakan model sequential.

4. Validation set sebesar 20% dari total dataset.

5. Harus menggunakan Embedding.

6. Harus menggunakan fungsi tokenizer.

7. Akurasi dari model minimal 75% pada train set dan validation set.

Klik link untuk mengunduh [Dataset](https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection/data?sort=votes)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split

# text processing
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
stopwords.words('english')
import string
string.punctuation
from nltk.stem.porter import PorterStemmer
import re

# utils
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

"""## Understanding Dataset"""

fake = pd.read_csv('C:/Users/ASUS/Downloads/TUGASKU/Dicoding/Lintasarta/Project1-MembuatModelNLPdenganTensorFlow/fake.csv')
true = pd.read_csv('C:/Users/ASUS/Downloads/TUGASKU/Dicoding/Lintasarta/Project1-MembuatModelNLPdenganTensorFlow/true.csv')

fake['label'] = 1
true['label'] = 0
fake.head()

true.head()

df = pd.concat([fake, true])
df

df.label.value_counts()

df.isna().sum()

"""## Exploratory Data Analysis (EDA)"""

df.head()

# shuffle dataset karena penggabungannya tidak langsung acak
df = df.sample(frac=1).reset_index(drop=True)
df

"""## Data Pre-Processing"""

df.drop(['date', 'subject'], axis=1, inplace=True)
df.head()

"""### Teks Processing"""

string.punctuation

def remove_Stopwords(text ):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize( text.lower() )
    sentence = [w for w in words if not w in stop_words]
    return " ".join(sentence)


def lemmatize_text(text):
    wordlist=[]
    lemmatizer = WordNetLemmatizer()
    sentences=sent_tokenize(text)
    for sentence in sentences:
        words=word_tokenize(sentence)
        for word in words:
            wordlist.append(lemmatizer.lemmatize(word))
    return ' '.join(wordlist)

def clean_text(text):
    delete_dict = {sp_character: '' for sp_character in string.punctuation}
    delete_dict[' '] = ' '
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    # Remove words starting with \
    text1 = re.sub(r'\\\w*', '', text1)
    textArr= text1.split()
    text2 = ' '.join([w for w in textArr])

    return text2.lower()

def stemSentence(text):
    porter = PorterStemmer()
    token_words=word_tokenize(text)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

df['new_text'] = df['text'].apply(clean_text)
df['new_text'] = df['new_text'].apply(remove_Stopwords)
df['new_text'] = df['new_text'].apply(lemmatize_text)
df['stem_text'] = df['new_text'].apply(stemSentence)

df

df.text[0]

df.new_text[0]

df.stem_text[0]

"""## Model Building"""

length = df['stem_text'].str.len().max()
length, df.columns

news = df['stem_text'].values
label = df['label'].values
label, news

news_train, news_test, label_train, label_test = train_test_split(news, label, test_size = 0.2, random_state = 123)

tokenizer = Tokenizer(num_words = 10000, oov_token = '<OOV>')
tokenizer.fit_on_texts(news_train)
tokenizer.fit_on_texts(news_test)

sequences_train = tokenizer.texts_to_sequences(news_train)
sequences_test = tokenizer.texts_to_sequences(news_test)

# maxlen = max(len(sequence) for sequence in sequences_train)
maxlen = int(sum(len(sequence) for sequence in sequences_train) / len(sequences_train))
# maxlen = int(np.percentile([len(sequence) for sequence in sequences_train], 90))
maxlen

"""## Pad Sequence"""

padded_train = pad_sequences(sequences=sequences_train,
                             maxlen = maxlen,
                             padding = 'post',
                             truncating = 'post')
padded_test = pad_sequences(sequences=sequences_test,
                            maxlen = maxlen,
                            padding = 'post',
                            truncating = 'post')

padded_train

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=256, input_length=maxlen),
    tf.keras.layers.LSTM(512),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

#compile
model.compile(loss ='binary_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

#cek model
model.summary()

#Callback Function
class accCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.90 and logs.get('val_accuracy') >= 0.90):
            print("\nAccuracy and Val_Accuracy has reached 98%!", "\nEpoch: ", epoch)
            self.model.stop_training = True

callbacks = accCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.2,
    min_lr = 0.000003
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 5,
    verbose = 1,
    mode = 'auto'
)

#latih model
history = model.fit(padded_train, label_train,
                    epochs = 100,
                    batch_size=128,
                    validation_data = (padded_test, label_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )

#plotting
pd.DataFrame(history.history).plot(figsize=(7, 4))
plt.grid(True)
plt.gca().set_ylim(0,3) #sumbu y

plt.show()

# plot loss
plt.subplot(211)
plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()

# plot accuracy
plt.subplot(212)
plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

